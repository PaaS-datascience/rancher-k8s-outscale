---
  - name: Launching instances for Rancher cluster
    hosts: localhost
    gather_facts: False
    become_user: root
    vars_files:
      - vars/rancher_cluster.yml

    tasks:
      - name: Which aws cli
        shell: which aws
        register: aws_location

      - name: Generating local uuid
        shell: /usr/bin/uuidgen
        register: uuid

      - name: Remove any previously generated key
        file:
          path: /tmp/rancher_id_rsa
          state: absent

      - name: Generate Rancher Cluster Keypair
        shell: >
          {{ aws_location.stdout }} ec2 create-key-pair --profile {{ aws_profile }}
          --key-name rancher-cluster-{{ uuid.stdout }}
          --endpoint {{ outscale_endpoint }}
          --query 'KeyMaterial'
          --output text > /tmp/rancher_id_rsa_{{ uuid.stdout }}
        register: cluster_keypair

      
      - name: Hard link key to /tmp/rancher_id_rsa
        file:
          src: /tmp/rancher_id_rsa_{{ uuid.stdout }}
          dest: /tmp/rancher_id_rsa
          mode: 0600
          state: hard

      - name: Set mode on hard link
        file:
          path: /tmp/rancher_id_rsa
          src: /tmp/rancher_id_rsa
          mode: 0600

      - debug: msg="{{ cluster_keypair }}"

      # ports to add toself 2379-2380
      # 10250
      - name: Generate Rancher Cluster Security Group
        shell: >
          {{ aws_location.stdout }} ec2 create-security-group --profile {{ aws_profile }}
          --group-name rancher-cluster-{{ uuid.stdout }}
          --description "Rancher Cluster"
          --endpoint {{ outscale_endpoint }}
          --output json | jq -r .GroupId
        register: secgroup

      - name: Applying default ingress for all nodes
        shell: >
          {{ aws_location.stdout }} ec2 authorize-security-group-ingress --profile {{ aws_profile }}
          --group-name rancher-cluster-{{ uuid.stdout }} --region "{{ outscale_region }}"
          --endpoint-url "{{ outscale_endpoint }}"
          --cli-input-json file://"{{ security_ingress }}"

      - name: Launch Outscale instances
        shell: >
          {{ aws_location.stdout }} ec2 run-instances 
          --profile "{{ aws_profile }}"
          --image-id "{{ outscale_rancher_image }}"
          --key-name "rancher-cluster-{{ uuid.stdout }}"
          --instance-type "{{ outscale_instance_type }}"
          --count "{{ rancher_node_count }}"
          --endpoint "{{ outscale_endpoint }}"
          --security-group-ids "{{ secgroup.stdout }}"
          --output json | jq -r .Instances[].InstanceId
        register: fcu_instance

      - debug: msg="{{ fcu_instance }}"

      - name: Remove file (delete file)
        file:
          path: /tmp/{{ item }}
          state: absent
        loop:
          - outscale.instance.ids
          - outscale.instance.public.ips
          - outscale.instance.private.ips

      - name: Write metadata to temp file
        lineinfile:
          path: /tmp/outscale.instance.ids
          line: "{{ fcu_instance.stdout_lines | to_json }}"
          create: yes
          mode: '0700'

      - name: Collect instance ids
        debug: msg="{{ lookup('file', '/tmp/outscale.instance.ids') }}"
        register: rancher_instances

      - debug: msg="{{ rancher_instances.msg }}"

      - name: Need to run describe to get public ips
        shell: >
              {{ aws_location.stdout }} ec2 describe-instances
              --profile "{{ aws_profile }}"
              --instance-id "{{ item }}"
              --endpoint "{{ outscale_endpoint }}"
              --output json | jq -r .Reservations[].Instances[].PublicIpAddress >> /tmp/outscale.instance.public.ips
        loop: "{{ rancher_instances.msg }}"
      
      - name: Need to run describe to get private ips
        shell: >
              {{ aws_location.stdout }} ec2 describe-instances
              --profile "{{ aws_profile }}"
              --instance-id "{{ item }}"
              --endpoint "{{ outscale_endpoint }}"
              --output json | jq -r .Reservations[].Instances[].PrivateIpAddress >> /tmp/outscale.instance.private.ips
        loop: "{{ rancher_instances.msg }}"  

      - name: Collect instance public ips
        shell: /bin/cat /tmp/outscale.instance.public.ips
        register: rancher_public_ips

      - name: Collect instance private ips
        shell: /bin/cat /tmp/outscale.instance.private.ips
        register: rancher_private_ips

      - debug: msg="{{ rancher_public_ips.stdout_lines }}"

      - name: Adding rancher public ips
        add_host:
          hostname: "{{ item }}"
          groupname: rancher_public
        with_items:
          - "{{ rancher_public_ips.stdout_lines }}"
      
      - name: Adding rancher private ips
        add_host:
          hostname: "{{ item }}"
          groupname: rancher_private
        with_items:
          - "{{ rancher_private_ips.stdout_lines }}"

      - set_fact:
          rancher_public_ips: "{{ rancher_public_ips.stdout_lines }}"

      - set_fact:
          rancher_private_ips: "{{ rancher_private_ips.stdout_lines }}"

      - name: Set updated ingress template
        template:
          src: templates/rancher-fcu-nodes-ingress.json.j2
          dest: /tmp/rancher-fcu-nodes-ingress.json
        delegate_to: localhost

      - name: Updating ingress for all nodes
        shell: >
          {{ aws_location.stdout }} ec2 authorize-security-group-ingress --profile {{ aws_profile }}
          --group-name rancher-cluster-{{ uuid.stdout }} --region "{{ outscale_region }}"
          --endpoint-url "{{ outscale_endpoint }}"
          --cli-input-json file:///tmp/rancher-fcu-nodes-ingress.json

      - name: Waiting for ssh
        wait_for:
          host: "{{ item }}"
          port: 22
          delay: 5
          timeout: 60
          state: started
        with_items:
          - "{{ rancher_public_ips }}"

  - name: Rancher node configuration
    hosts: rancher_public
    gather_facts: false
    environment:
      KUBECONFIG: /home/outscale/kube_config_cluster.yml
    become: yes
    become_user: root
    vars_files:
      - vars/rancher_cluster.yml

    pre_tasks:
    - name: Install inotifytools
      raw: test -e /usr/bin/python3 || (rm /var/lib/apt/lists/lock && rm /var/lib/dpkg/lock && apt install -qqy inotify-tools)
      register: output
      changed_when: output.stdout != ""

    tasks:

      - name: Let's make sure there are no lock issues with apt
        shell: /usr/bin/inotifywait -q -e close /var/lib/dpkg/lock -t 60 ; /bin/rm /var/lib/dpkg/lock

      - name: Updating apt
        apt: 
          upgrade: yes
          update_cache: yes
        retries: 5
        delay: 20
        register: ecode
        until: ecode is success

      - name: Add generated rsa key to /home/outscale for initial RKE spinup
        copy:
          src: /tmp/rancher_id_rsa
          dest: /home/outscale/rancher_id_rsa
          owner: outscale
          group: outscale
          mode: 0644

      - name: Install docker
        apt:
          state: present
          pkg: docker.io

      - name: Install awscli
        apt:
          state: present
          pkg: awscli

      - name: Add outscale user to docker group on each node
        user:
          name: outscale
          groups: docker
          append: yes

      - name: Download RKE
        get_url:
          url: https://github.com/rancher/rke/releases/download/v1.0.4/rke_linux-amd64
          dest: /usr/local/bin/rke

      - name: Rename RKE and +x the bin
        shell: >
          /bin/chmod +x /usr/local/bin/rke

      - debug: msg="{{ hostvars }}"

      - name: Set cluster template
        template:
          src: templates/rancher-cluster.j2
          dest: /home/outscale/cluster.yml

      - name: Spinup initial K8S cluster; first idempotent run will fail
        shell: >
          /usr/local/bin/rke up --config /home/outscale/cluster.yml
        args:
          chdir: /home/outscale
        ignore_errors: yes 
        retries: 5
        delay: 300
        delegate_to: "{{ hostvars[groups['rancher_public'][0]]['inventory_hostname'] }}"

      - name: Install Helm
        snap:
          name: "{{ item }}"
          classic: yes
          state: present
        loop:
          - helm
          - kubectl

      - name: Add rancher repo
        shell: /snap/bin/helm repo add rancher-stable https://releases.rancher.com/server-charts/stable

      - name: Update helm repo
        shell: /snap/bin/helm repo update

      - name: Create namespace
        shell: /snap/bin/kubectl create namespace {{ k8s_cluster_namespace }}
        environment:
          KUBECONFIG: /home/outscale/kube_config_cluster.yml

      - name: Install Rancher
        shell: /snap/bin/helm install rancher rancher-stable/rancher \
          --namespace {{ k8s_cluster_namespace}} \
          --set hostname={{ k8s_cluster_hostname }}-k8s.imedidata.net
          --set ingress.tls.source=secret

      - name: Download Rancher CLI onto all nodes
        get_url:
          url: https://github.com/rancher/cli/releases/download/v2.3.2/rancher-linux-amd64-v2.3.2.tar.xz
          dest: /tmp/rancher-linux-amd64-v2.3.2.tar.xz

      - name: Unarchive Rancher 2.3.2 
        unarchive:
          src: /tmp/rancher-linux-amd64-v2.3.2.tar.xz
          dest: /usr/local
          remote_src: yes

      # Wouldn't it have been nice to have that vault.imedidata.net to pull certs via curl?
      # Now we have to do it like this

      # Creating a bucket is as follows; Outscale documentation is incorrect
      # aws s3 mb s3://cloud-operations --profile outscale --endpoint https://osu.us-east-2.outscale.com

      # Adding something to the cloud-operations bucket
      # aws s3 cp --profile outscale imedidata.net.certs.tar s3://cloud-operations --endpoint https://osu.us-east-2.outscale.com

      - name: Which aws cli
        shell: which aws
        register: aws_location
        delegate_to: localhost
        run_once: true
        become: no
      
      - name: Remove existing archive if it is exists
        file:
          path: /tmp/imedidata.net.certs.tar
          state: absent
        delegate_to: localhost
        run_once: true
        become: no
      
      - name: Pull from Outscale Storage Unit
        shell: >
               {{ aws_location.stdout }} s3api get-object --profile {{ aws_profile }} \
               --bucket cloud-operations \
               --key imedidata.net.certs.tar /tmp/imedidata.net.certs.tar \
               --endpoint {{ outscale_object_endpoint }}
        delegate_to: localhost
        run_once: true
        become: no

      - name: Copy /tmp/imedidata.net.certs.tar to nodes
        copy:
          src: /tmp/imedidata.net.certs.tar 
          dest: /tmp/imedidata.net.certs.tar 

      - name: Place *.imedidata.net certs
        shell: |
           /bin/tar xvf imedidata.net.certs.tar
           /bin/mv key.key /etc/ssl/private/imedidata.key
           /bin/mv crt.crt /etc/ssl/private/imedidata.crt
           /bin/rm /tmp/combined.crt
           /bin/rm /tmp/imedidata.net.certs.tar
           /bin/chmod 0644 /etc/ssl/private/imedidata.???
        args:
          chdir: /tmp

      - name: Delete existing generated certs
        shell: >
          /snap/bin/kubectl -n {{ k8s_cluster_namespace }} delete secret tls-rancher-ingress
        ignore_errors: yes

      - name: Add TLS certs to Rancher
        shell: >
          /snap/bin/kubectl -n {{ k8s_cluster_namespace }} create secret tls tls-rancher-ingress \
          --cert=/etc/ssl/private/imedidata.crt \
          --key=/etc/ssl/private/imedidata.key
        retries: 5
        delay: 60
        register: ecode
        until: ecode is success

  - name: Wait for rancher to become available
    hosts: localhost
    become_user: root
    gather_facts: false
    vars_files:
      - vars/rancher_cluster.yml
    tasks:

      - name: Collect instance ips
        shell: /bin/cat /tmp/outscale.instance.public.ips
        register: rancher_public_ips

      - name: Provide instance IP for login and initial setup
        debug: 
          msg: "Please login to https://{{ rancher_public_ips.stdout_lines[0] }} and finish configuration for initial Rancher cluster setup"

      - name: Set route53 change-resource-record-sets template
        template:
          src: templates/rancher-cluster-route53.j2
          dest: /tmp/rancher-cluster-route53.json
        delegate_to: localhost

      # Z1G33JXZ3XRD06 is the hosted zone id in route53 for imedidata.net
      # Lets leave this hard coded for now
      - name: Creating A record via route53 using route53 aws profile
        shell: >
         {{ aws_location.stdout }} route53 change-resource-record-sets --profile {{ route53_profile }} \
         --hosted-zone-id Z1G33JXZ3XRD06 \
         --change-batch file:///tmp/rancher-cluster-route53.json

      # Setup an API Key via the UI
      # Copy the bearer token and configure rancher cli
      # Use rancher cli to setup Okta
    
      #- name: Set rancher settings
      #  shell:
      #    /usr/local/rancher-v2.3.2/rancher settings set ui-pl medidata
      #    /usr/local/rancher-v2.3.2/rancher settings set server-url https://{{ k8s_cluster_hostname }}-k8s.imedidata.net
      #    /usr/local/rancher-v2.3.2/rancher settings set ingress-ip-domain imedidata.net
      #  delegate_to: "{{ hostvars[groups['rancher_public'][0]]['inventory_hostname'] }}"

      - name: Subsequent IPS for configuration
        debug:
          msg: "{{ rancher_public_ips.stdout_lines }}"

      # Outscale Load Balancer created using Cockpit
      # LB support via Outscale isn't a mature service
      # This is disabled for now, round robin via Route 53
      # will be used in place
      #- name: Registering instances with load balancer
      #  shell: >
      #         {{ aws_location.stdout }} elb register-instances-with-load-balancer \
      #         --profile "{{ aws_profile }}" \
      #         --load-balancer-name cloudinfra-rancher-k8s \
      #         --instances {{ item }} \
      #         --endpoint https://lbu.us-east-2.outscale.com/
      #  loop: "{{ rancher_instances.msg }}" 
      
      - name: Waiting for port 80
        wait_for:
          host: "{{ rancher_public_ips.stdout_lines[0] }}"
          port: 80
          delay: 60
          timeout: 120
          state: started